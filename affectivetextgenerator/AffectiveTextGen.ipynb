{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ishikasingh/Affective-text-gen/blob/master/AffectiveTextGen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "kIEkru9a89ev",
    "outputId": "48703456-310b-4984-b3cd-9c976261c4dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/haraldosterling/anaconda3/lib/python3.8/site-packages (4.6.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/haraldosterling/anaconda3/lib/python3.8/site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: packaging in /Users/haraldosterling/anaconda3/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/haraldosterling/anaconda3/lib/python3.8/site-packages (from transformers) (1.20.2)\n",
      "Requirement already satisfied: filelock in /Users/haraldosterling/anaconda3/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in /Users/haraldosterling/anaconda3/lib/python3.8/site-packages (from transformers) (0.0.8)\n",
      "Requirement already satisfied: requests in /Users/haraldosterling/anaconda3/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: sacremoses in /Users/haraldosterling/anaconda3/lib/python3.8/site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/haraldosterling/anaconda3/lib/python3.8/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /Users/haraldosterling/anaconda3/lib/python3.8/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/haraldosterling/anaconda3/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/haraldosterling/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/haraldosterling/anaconda3/lib/python3.8/site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/haraldosterling/anaconda3/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/haraldosterling/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: click in /Users/haraldosterling/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (8.0.1)\n",
      "Requirement already satisfied: joblib in /Users/haraldosterling/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /Users/haraldosterling/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "#### Code based on PPLM framework ####\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "09MUYiLt9wta"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from operator import add\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm import trange\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers.file_utils import cached_path\n",
    "from transformers import GPT2LMHeadModel\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "M1TvKXs09QVG"
   },
   "outputs": [],
   "source": [
    "class ClassificationHead(torch.nn.Module):\n",
    "    \"\"\"Classification Head for  transformer encoders\"\"\"\n",
    "\n",
    "    def __init__(self, class_size, embed_size):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.class_size = class_size\n",
    "        self.embed_size = embed_size\n",
    "        # self.mlp1 = torch.nn.Linear(embed_size, embed_size)\n",
    "        # self.mlp2 = (torch.nn.Linear(embed_size, class_size))\n",
    "        self.mlp = torch.nn.Linear(embed_size, class_size)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        # hidden_state = F.relu(self.mlp1(hidden_state))\n",
    "        # hidden_state = self.mlp2(hidden_state)\n",
    "        logits = self.mlp(hidden_state)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "EX4W7SCz94Q9"
   },
   "outputs": [],
   "source": [
    "PPLM_BOW = 1\n",
    "PPLM_DISCRIM = 2\n",
    "PPLM_BOW_DISCRIM = 3\n",
    "BOW_AFFECT = 4\n",
    "SMALL_CONST = 1e-15\n",
    "BIG_CONST = 1e10\n",
    "\n",
    "QUIET = 0\n",
    "REGULAR = 1\n",
    "VERBOSE = 2\n",
    "VERY_VERBOSE = 3\n",
    "VERBOSITY_LEVELS = {\n",
    "    'quiet': QUIET,\n",
    "    'regular': REGULAR,\n",
    "    'verbose': VERBOSE,\n",
    "    'very_verbose': VERY_VERBOSE,\n",
    "}\n",
    "\n",
    "BAG_OF_WORDS_ARCHIVE_MAP = {\n",
    "    'legal': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/legal.txt\",\n",
    "    'military': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/military.txt\",\n",
    "    'monsters': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/monsters.txt\",\n",
    "    'politics': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/politics.txt\",\n",
    "    'positive_words': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/positive_words.txt\",\n",
    "    'religion': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/religion.txt\",\n",
    "    'science': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/science.txt\",\n",
    "    'space': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/space.txt\",\n",
    "    'technology': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/technology.txt\",\n",
    "}\n",
    "\n",
    "DISCRIMINATOR_MODELS_PARAMS = {\n",
    "    \"clickbait\": {\n",
    "        \"url\": \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/discriminators/clickbait_classifier_head.pt\",\n",
    "        \"class_size\": 2,\n",
    "        \"embed_size\": 1024,\n",
    "        \"class_vocab\": {\"non_clickbait\": 0, \"clickbait\": 1},\n",
    "        \"default_class\": 1,\n",
    "        \"pretrained_model\": \"gpt2-medium\",\n",
    "    },\n",
    "    \"sentiment\": {\n",
    "        \"url\": \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/discriminators/SST_classifier_head.pt\",\n",
    "        \"class_size\": 5,\n",
    "        \"embed_size\": 1024,\n",
    "        \"class_vocab\": {\"very_positive\": 2, \"very_negative\": 3},\n",
    "        \"default_class\": 3,\n",
    "        \"pretrained_model\": \"gpt2-medium\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "503RMAOx976u"
   },
   "outputs": [],
   "source": [
    "def to_var(x, requires_grad=False, volatile=False, device='cuda'):\n",
    "    if torch.cuda.is_available() and device == 'cuda':\n",
    "        x = x.cuda()\n",
    "    elif device != 'cuda':\n",
    "        x = x.to(device)\n",
    "    return Variable(x, requires_grad=requires_grad, volatile=volatile)\n",
    "\n",
    "\n",
    "def top_k_filter(logits, k, probs=False):\n",
    "    \"\"\"\n",
    "    Masks everything but the k top entries as -infinity (1e10).\n",
    "    Used to mask logits such that e^-infinity -> 0 won't contribute to the\n",
    "    sum of the denominator.\n",
    "    \"\"\"\n",
    "    if k == 0:\n",
    "        return logits\n",
    "    else:\n",
    "        values = torch.topk(logits, k)[0]\n",
    "        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\n",
    "        if probs:\n",
    "            return torch.where(logits < batch_mins,\n",
    "                               torch.ones_like(logits) * 0.0, logits)\n",
    "        return torch.where(logits < batch_mins,\n",
    "                           torch.ones_like(logits) * -BIG_CONST,\n",
    "                           logits)\n",
    "\n",
    "def gaussian(x, mu, sig):\n",
    "  x = np.array(x)\n",
    "  return list(np.exp(-0.5*((x-mu)/sig)**2)/(sig*(2*np.pi)**0.5))\n",
    "\n",
    "def perturb_past(\n",
    "        past,\n",
    "        model,\n",
    "        last,\n",
    "        affect_weight=0.2,\n",
    "        unpert_past=None,\n",
    "        unpert_logits=None,\n",
    "        accumulated_hidden=None,\n",
    "        grad_norms=None,\n",
    "        stepsize=0.01,\n",
    "        one_hot_bows_vectors=None,\n",
    "        one_hot_bows_affect=None,\n",
    "        affect_int = None,\n",
    "        knob = None,\n",
    "        classifier=None,\n",
    "        class_label=None,\n",
    "        loss_type=0,\n",
    "        num_iterations=3,\n",
    "        horizon_length=1,\n",
    "        window_length=0,\n",
    "        decay=False,\n",
    "        gamma=1.5,\n",
    "        kl_scale=0.01,\n",
    "        device='cuda',\n",
    "        verbosity_level=REGULAR\n",
    "):\n",
    "    # Generate inital perturbed past\n",
    "    grad_accumulator = [\n",
    "        (np.zeros(p.shape).astype(\"float32\"))\n",
    "        for p in past\n",
    "    ]\n",
    "    \n",
    "    if accumulated_hidden is None:\n",
    "        accumulated_hidden = 0\n",
    "\n",
    "    if decay:\n",
    "        decay_mask = torch.arange(\n",
    "            0.,\n",
    "            1.0 + SMALL_CONST,\n",
    "            1.0 / (window_length)\n",
    "        )[1:]\n",
    "    else:\n",
    "        decay_mask = 1.0\n",
    "\n",
    "    # TODO fix this comment (SUMANTH)\n",
    "    # Generate a mask is gradient perturbated is based on a past window\n",
    "    _, _, _, curr_length, _ = past[0].shape\n",
    "\n",
    "    if curr_length > window_length and window_length > 0:\n",
    "        ones_key_val_shape = (\n",
    "                tuple(past[0].shape[:-2])\n",
    "                + tuple([window_length])\n",
    "                + tuple(past[0].shape[-1:])\n",
    "        )\n",
    "\n",
    "        zeros_key_val_shape = (\n",
    "                tuple(past[0].shape[:-2])\n",
    "                + tuple([curr_length - window_length])\n",
    "                + tuple(past[0].shape[-1:])\n",
    "        )\n",
    "\n",
    "        ones_mask = torch.ones(ones_key_val_shape)\n",
    "        ones_mask = decay_mask * ones_mask.permute(0, 1, 2, 4, 3)\n",
    "        ones_mask = ones_mask.permute(0, 1, 2, 4, 3)\n",
    "\n",
    "        window_mask = torch.cat(\n",
    "            (ones_mask, torch.zeros(zeros_key_val_shape)),\n",
    "            dim=-2\n",
    "        ).to(device)\n",
    "    else:\n",
    "        window_mask = torch.ones_like(past[0]).to(device)\n",
    "\n",
    "    # accumulate perturbations for num_iterations\n",
    "    loss_per_iter = []\n",
    "    new_accumulated_hidden = None\n",
    "    for i in range(num_iterations):\n",
    "        if verbosity_level >= VERBOSE:\n",
    "            print(\"Iteration \", i + 1)\n",
    "        curr_perturbation = [\n",
    "            to_var(torch.from_numpy(p_), requires_grad=True, device=device)\n",
    "            for p_ in grad_accumulator\n",
    "        ]\n",
    "\n",
    "        # Compute hidden using perturbed past\n",
    "        perturbed_past = list(map(add, past, curr_perturbation))\n",
    "        _, _, _, curr_length, _ = curr_perturbation[0].shape\n",
    "        all_logits, _, all_hidden = model(last, past=perturbed_past)\n",
    "        hidden = all_hidden[-1]\n",
    "        new_accumulated_hidden = accumulated_hidden + torch.sum(\n",
    "            hidden,\n",
    "            dim=1\n",
    "        ).detach()\n",
    "        # TODO: Check the layer-norm consistency of this with trained discriminator (Sumanth)\n",
    "        logits = all_logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        loss = 0.0\n",
    "        loss_list = []\n",
    "        if loss_type == PPLM_BOW or loss_type == BOW_AFFECT:\n",
    "            for one_hot_bow in one_hot_bows_vectors:\n",
    "                bow_logits = torch.mm(probs, torch.t(one_hot_bow))\n",
    "                #print(type(bow_logits))\n",
    "                bow_loss = -torch.log(torch.sum(bow_logits))\n",
    "                #print(bow_loss)\n",
    "                loss +=  bow_loss\n",
    "                loss_list.append(bow_loss)\n",
    "            if loss_type == BOW_AFFECT:\n",
    "              for one_hot_bow in one_hot_bows_affect:\n",
    "                  bow_logits = torch.mm(probs, torch.t(one_hot_bow))\n",
    "                 # print(bow_logits.size(), torch.FloatTensor(affect_int).size())\n",
    "                  bow_loss = -torch.log(torch.matmul(bow_logits, torch.t(torch.FloatTensor(gaussian(affect_int, knob, .1)).to(device))))#-torch.log(torch.sum(bow_logits))#\n",
    "                  # print(bow_loss)\n",
    "\n",
    "                  loss += affect_weight * bow_loss[0]\n",
    "                  loss_list.append(bow_loss)\n",
    "            if verbosity_level >= VERY_VERBOSE:\n",
    "                print(\" pplm_bow_loss:\", loss.data.cpu().numpy())\n",
    "\n",
    "        kl_loss = 0.0\n",
    "        if kl_scale > 0.0:\n",
    "            unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\n",
    "            unpert_probs = (\n",
    "                    unpert_probs + SMALL_CONST *\n",
    "                    (unpert_probs <= SMALL_CONST).float().to(device).detach()\n",
    "            )\n",
    "            correction = SMALL_CONST * (probs <= SMALL_CONST).float().to(\n",
    "                device).detach()\n",
    "            corrected_probs = probs + correction.detach()\n",
    "            kl_loss = kl_scale * (\n",
    "                (corrected_probs * (corrected_probs / unpert_probs).log()).sum()\n",
    "            )\n",
    "            if verbosity_level >= VERY_VERBOSE:\n",
    "                print(' kl_loss', kl_loss.data.cpu().numpy())\n",
    "            loss += kl_loss\n",
    "\n",
    "        loss_per_iter.append(loss.data.cpu().numpy())\n",
    "        if verbosity_level >= VERBOSE:\n",
    "            print(' pplm_loss', (loss - kl_loss).data.cpu().numpy())\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # calculate gradient norms\n",
    "        if grad_norms is not None and loss_type == PPLM_BOW:\n",
    "            grad_norms = [\n",
    "                torch.max(grad_norms[index], torch.norm(p_.grad * window_mask))\n",
    "                for index, p_ in enumerate(curr_perturbation)\n",
    "            ]\n",
    "        else:\n",
    "            grad_norms = [\n",
    "                (torch.norm(p_.grad * window_mask) + SMALL_CONST)\n",
    "                for index, p_ in enumerate(curr_perturbation)\n",
    "            ]\n",
    "\n",
    "        # normalize gradients\n",
    "        grad = [\n",
    "            -stepsize *\n",
    "            (p_.grad * window_mask / grad_norms[\n",
    "                index] ** gamma).data.cpu().numpy()\n",
    "            for index, p_ in enumerate(curr_perturbation)\n",
    "        ]\n",
    "\n",
    "        # accumulate gradient\n",
    "        grad_accumulator = list(map(add, grad, grad_accumulator))\n",
    "\n",
    "        # reset gradients, just to make sure\n",
    "        for p_ in curr_perturbation:\n",
    "            p_.grad.data.zero_()\n",
    "\n",
    "        # removing past from the graph\n",
    "        new_past = []\n",
    "        for p_ in past:\n",
    "            new_past.append(p_.detach())\n",
    "        past = new_past\n",
    "\n",
    "    # apply the accumulated perturbations to the past\n",
    "    grad_accumulator = [\n",
    "        to_var(torch.from_numpy(p_), requires_grad=True, device=device)\n",
    "        for p_ in grad_accumulator\n",
    "    ]\n",
    "    pert_past = list(map(add, past, grad_accumulator))\n",
    "\n",
    "    return pert_past, new_accumulated_hidden, grad_norms, loss_per_iter\n",
    "\n",
    "\n",
    "def get_classifier(\n",
    "        name: Optional[str],\n",
    "        class_label: Union[str, int],\n",
    "        device: str,\n",
    "        verbosity_level: int = REGULAR\n",
    ") -> Tuple[Optional[ClassificationHead], Optional[int]]:\n",
    "    if name is None:\n",
    "        return None, None\n",
    "\n",
    "    params = DISCRIMINATOR_MODELS_PARAMS[name]\n",
    "    classifier = ClassificationHead(\n",
    "        class_size=params['class_size'],\n",
    "        embed_size=params['embed_size']\n",
    "    ).to(device)\n",
    "    if \"url\" in params:\n",
    "        resolved_archive_file = cached_path(params[\"url\"])\n",
    "    elif \"path\" in params:\n",
    "        resolved_archive_file = params[\"path\"]\n",
    "    else:\n",
    "        raise ValueError(\"Either url or path have to be specified \"\n",
    "                         \"in the discriminator model parameters\")\n",
    "    classifier.load_state_dict(\n",
    "        torch.load(resolved_archive_file, map_location=device))\n",
    "    classifier.eval()\n",
    "\n",
    "    if isinstance(class_label, str):\n",
    "        if class_label in params[\"class_vocab\"]:\n",
    "            label_id = params[\"class_vocab\"][class_label]\n",
    "        else:\n",
    "            label_id = params[\"default_class\"]\n",
    "            if verbosity_level >= REGULAR:\n",
    "                print(\"class_label {} not in class_vocab\".format(class_label))\n",
    "                print(\"available values are: {}\".format(params[\"class_vocab\"]))\n",
    "                print(\"using default class {}\".format(label_id))\n",
    "\n",
    "    elif isinstance(class_label, int):\n",
    "        if class_label in set(params[\"class_vocab\"].values()):\n",
    "            label_id = class_label\n",
    "        else:\n",
    "            label_id = params[\"default_class\"]\n",
    "            if verbosity_level >= REGULAR:\n",
    "                print(\"class_label {} not in class_vocab\".format(class_label))\n",
    "                print(\"available values are: {}\".format(params[\"class_vocab\"]))\n",
    "                print(\"using default class {}\".format(label_id))\n",
    "\n",
    "    else:\n",
    "        label_id = params[\"default_class\"]\n",
    "\n",
    "    return classifier, label_id\n",
    "\n",
    "\n",
    "def get_bag_of_words_indices(bag_of_words_ids_or_paths: List[str], tokenizer) -> \\\n",
    "        List[List[List[int]]]:\n",
    "    bow_indices = []\n",
    "    for id_or_path in bag_of_words_ids_or_paths:\n",
    "        if id_or_path in BAG_OF_WORDS_ARCHIVE_MAP:\n",
    "            filepath = cached_path(BAG_OF_WORDS_ARCHIVE_MAP[id_or_path])\n",
    "        else:\n",
    "            filepath = id_or_path\n",
    "        with open(filepath, \"r\") as f:\n",
    "            words = f.read().strip().split(\"\\n\")\n",
    "        bow_indices.append(\n",
    "            [tokenizer.encode(word.strip(),\n",
    "                              add_prefix_space=True,\n",
    "                              add_special_tokens=False)\n",
    "             for word in words])\n",
    "    return bow_indices\n",
    "\n",
    "def get_affect_words_and_int (affect_class):\n",
    "  emotions = \"https://raw.githubusercontent.com/ishikasingh/Affective-text-gen/master/NRC-Emotion-Intensity-Lexicon-v1.txt\"\n",
    "  filepath = cached_path(emotions)\n",
    "  with open(filepath, \"r\") as f:\n",
    "      words = f.read().strip().split(\"\\n\")[1:]\n",
    "  words = [w.split(\"\\t\") for w in words]\n",
    "  return [w[0] for w in words if w[1] == affect_class], [float(w[-1]) for w in words if w[1] == affect_class]\n",
    "\n",
    "def build_bows_one_hot_vectors(bow_indices, tokenizer, device='cuda'):\n",
    "    if bow_indices is None:\n",
    "        return None\n",
    "\n",
    "    one_hot_bows_vectors = []\n",
    "    for single_bow in bow_indices:\n",
    "        single_bow = list(filter(lambda x: len(x) <= 1, single_bow))\n",
    "\n",
    "        single_bow = torch.tensor(single_bow).to(device)\n",
    "        num_words = single_bow.shape[0]\n",
    "        # print(num_words)\n",
    "        one_hot_bow = torch.zeros(num_words, tokenizer.vocab_size).to(device)\n",
    "        one_hot_bow.scatter_(1, single_bow, 1)\n",
    "        one_hot_bows_vectors.append(one_hot_bow)\n",
    "    return one_hot_bows_vectors\n",
    "\n",
    "def build_bows_one_hot_vectors_aff(bow_indices,affect_int, tokenizer, device='cuda'):\n",
    "    if bow_indices is None or affect_int is None:\n",
    "        return None, None\n",
    "\n",
    "    one_hot_bows_vectors = []\n",
    "    # print(np.array(bow_indices).shape)\n",
    "    for single_bow in bow_indices:\n",
    "        zipped = [[single_bow[i], affect_int[i]] for i in range(len(single_bow))]\n",
    "        single_bow_int = list(filter(lambda x: len(x[0]) <= 1, zipped))\n",
    "        single_bow = [single_bow_int[i][0] for i in range(len(single_bow_int)) ]\n",
    "        affect_ints = [single_bow_int[i][1] for i in range(len(single_bow_int)) ]\n",
    "        # print(single_bow, affect_ints)\n",
    "        # print(len(single_bow), len(affect_ints))\n",
    "        single_bow = torch.tensor(single_bow).to(device)\n",
    "        num_words = single_bow.shape[0]\n",
    "        # print(num_words)\n",
    "        one_hot_bow = torch.zeros(num_words, tokenizer.vocab_size).to(device)\n",
    "        one_hot_bow.scatter_(1, single_bow, 1)\n",
    "        one_hot_bows_vectors.append(one_hot_bow)\n",
    "    return one_hot_bows_vectors, affect_ints\n",
    "\n",
    "\n",
    "\n",
    "def full_text_generation(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        affect_weight=0.2,\n",
    "        knob = None,\n",
    "        context=None,\n",
    "        num_samples=1,\n",
    "        device=\"cuda\",\n",
    "        bag_of_words=None,\n",
    "        bag_of_words_affect=None,\n",
    "        discrim=None,\n",
    "        class_label=None,\n",
    "        length=100,\n",
    "        stepsize=0.02,\n",
    "        temperature=1.0,\n",
    "        top_k=10,\n",
    "        sample=True,\n",
    "        num_iterations=3,\n",
    "        grad_length=10000,\n",
    "        horizon_length=1,\n",
    "        window_length=0,\n",
    "        decay=False,\n",
    "        gamma=1.5,\n",
    "        gm_scale=0.9,\n",
    "        kl_scale=0.01,\n",
    "        verbosity_level=REGULAR,\n",
    "        **kwargs\n",
    "):\n",
    "    classifier, class_id = get_classifier(discrim, class_label, device)\n",
    "    # print(\"bog is here\", bag_of_words)\n",
    "    # print(\"affect is: \", bag_of_words_affect)\n",
    "    bow_indices = []\n",
    "    bow_indices_affect = []\n",
    "    if bag_of_words:\n",
    "      bow_indices = get_bag_of_words_indices(bag_of_words.split(\";\"), tokenizer)\n",
    "    if bag_of_words_affect: \n",
    "      affect_words, affect_int = get_affect_words_and_int(bag_of_words_affect)\n",
    "      bow_indices_affect.append([tokenizer.encode(word.strip(),add_prefix_space=True, add_special_tokens=False)for word in affect_words])\n",
    "    # print(\"aff1\", affect_int)\n",
    "    loss_type = PPLM_BOW\n",
    "    if bag_of_words_affect:\n",
    "      loss_type = BOW_AFFECT\n",
    "\n",
    "    unpert_gen_tok_text, _, _ = generate_text_pplm(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        context=context,\n",
    "        device=device,\n",
    "        length=length,\n",
    "        sample=sample,\n",
    "        perturb=False,\n",
    "        verbosity_level=verbosity_level\n",
    "    )\n",
    "\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    pert_gen_tok_texts = []\n",
    "    discrim_losses = []\n",
    "    losses_in_time = []\n",
    "    print(\"After Perturbation\")\n",
    "    for i in range(num_samples):\n",
    "        pert_gen_tok_text, discrim_loss, loss_in_time = generate_text_pplm(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            affect_weight=affect_weight,\n",
    "            context=context,\n",
    "            device=device,\n",
    "            perturb=True,\n",
    "            bow_indices=bow_indices,\n",
    "            bow_indices_affect=bow_indices_affect,\n",
    "            affect_int = affect_int,\n",
    "            knob = knob,\n",
    "            classifier=classifier,\n",
    "            class_label=class_id,\n",
    "            loss_type=loss_type,\n",
    "            length=length,\n",
    "            stepsize=stepsize,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            sample=sample,\n",
    "            num_iterations=num_iterations,\n",
    "            grad_length=grad_length,\n",
    "            horizon_length=horizon_length,\n",
    "            window_length=window_length,\n",
    "            decay=decay,\n",
    "            gamma=gamma,\n",
    "            gm_scale=gm_scale,\n",
    "            kl_scale=kl_scale,\n",
    "            verbosity_level=verbosity_level\n",
    "        )\n",
    "        pert_gen_tok_texts.append(pert_gen_tok_text)\n",
    "        if classifier is not None:\n",
    "            discrim_losses.append(discrim_loss.data.cpu().numpy())\n",
    "        losses_in_time.append(loss_in_time)\n",
    "\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "iVFnR5a2oIFj"
   },
   "outputs": [],
   "source": [
    "def get_affect_words_and_int1 (affect_class):\n",
    "  emotions = \"https://raw.githubusercontent.com/ishikasingh/Affective-text-gen/master/NRC-AffectIntensity-Lexicon.txt\"\n",
    "  filepath = cached_path(emotions)\n",
    "  with open(filepath, \"r\") as f:\n",
    "      words = f.read().strip().split(\"\\n\")[37:]\n",
    "  words = [w.split(\"\\t\") for w in words]\n",
    "  return [w[0] for w in words if w[-1] == affect_class], [float(w[1]) for w in words if w[-1] == affect_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "SprqNon6JuHj"
   },
   "outputs": [],
   "source": [
    "k, j = get_affect_words_and_int (\"anger\")\n",
    "k1, j1 = get_affect_words_and_int1 (\"anger\")\n",
    "\n",
    "# type(torch.matmul(torch.FloatTensor(j[:10]), torch.FloatTensor(j[:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "kp9rSoAKycDc",
    "outputId": "06eb7a0b-c0c8-44f6-831d-c5851b1cd197"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['outraged', 'brutality', 'hatred', 'hateful', 'terrorize'],\n",
       " ['outraged', 'brutality', 'hatred', 'hateful', 'terrorize'],\n",
       " [0.964, 0.959, 0.953, 0.94, 0.939],\n",
       " [0.964, 0.959, 0.953, 0.94, 0.939])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j==j1, k==k1\n",
    "k[:5], k1[:5], j[:5], j1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "eOG5q_jJ-Cxl"
   },
   "outputs": [],
   "source": [
    "def run_pplm_example(\n",
    "        pretrained_model=\"gpt2-medium\",\n",
    "        cond_text=\"\",\n",
    "        affect_weight=0.2,\n",
    "        knob = None,\n",
    "        uncond=False,\n",
    "        num_samples=1,\n",
    "        bag_of_words=None,\n",
    "        bag_of_words_affect=None,\n",
    "        discrim=None,\n",
    "        discrim_weights=None,\n",
    "        discrim_meta=None,\n",
    "        class_label=-1,\n",
    "        length=100,\n",
    "        stepsize=0.02,\n",
    "        temperature=1.0,\n",
    "        top_k=10,\n",
    "        sample=True,\n",
    "        num_iterations=3,\n",
    "        grad_length=10000,\n",
    "        horizon_length=1,\n",
    "        window_length=0,\n",
    "        decay=False,\n",
    "        gamma=1.5,\n",
    "        gm_scale=0.9,\n",
    "        kl_scale=0.01,\n",
    "        seed=0,\n",
    "        no_cuda=False,\n",
    "        colorama=False,\n",
    "        verbosity='regular'\n",
    "):\n",
    "    # set Random seed\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # set verbosiry\n",
    "    verbosity_level = VERBOSITY_LEVELS.get(verbosity.lower(), REGULAR)\n",
    "\n",
    "    # # set the device\n",
    "    # device = \"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\"\n",
    "\n",
    "    if discrim == 'generic':\n",
    "        set_generic_model_params(discrim_weights, discrim_meta)\n",
    "\n",
    "    if discrim is not None:\n",
    "        discriminator_pretrained_model = DISCRIMINATOR_MODELS_PARAMS[discrim][\n",
    "            \"pretrained_model\"\n",
    "        ]\n",
    "        if pretrained_model != discriminator_pretrained_model:\n",
    "            pretrained_model = discriminator_pretrained_model\n",
    "            if verbosity_level >= REGULAR:\n",
    "                print(\"discrim = {}, pretrained_model set \"\n",
    "                \"to discriminator's = {}\".format(discrim, pretrained_model))\n",
    "\n",
    "    # # load pretrained model\n",
    "    # model = GPT2LMHeadModel.from_pretrained(\n",
    "    #     pretrained_model,\n",
    "    #     output_hidden_states=True\n",
    "    # )\n",
    "    # model.to(device)\n",
    "    # model.eval()\n",
    "\n",
    "    # # load tokenizer\n",
    "    # tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)\n",
    "\n",
    "    # Freeze GPT-2 weights\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # figure out conditioning text\n",
    "    if uncond:\n",
    "        tokenized_cond_text = tokenizer.encode([tokenizer.bos_token],add_special_tokens=False)\n",
    "    else:\n",
    "        raw_text = cond_text\n",
    "        while not raw_text:\n",
    "            print(\"Did you forget to add `--cond_text`? \")\n",
    "            raw_text = input(\"Model prompt >>> \")\n",
    "        tokenized_cond_text = tokenizer.encode(tokenizer.bos_token + raw_text,add_special_tokens=False)\n",
    "    print(\"= Prefix of sentence =\")\n",
    "    # generate unperturbed and perturbed texts\n",
    "\n",
    "    # full_text_generation returns:\n",
    "    # unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time\n",
    "    unpert_gen_tok_text, pert_gen_tok_texts, _, _ = full_text_generation(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        affect_weight=affect_weight,\n",
    "        knob = knob,\n",
    "        context=tokenized_cond_text,\n",
    "        device=device,\n",
    "        num_samples=num_samples,\n",
    "        bag_of_words=bag_of_words,\n",
    "        bag_of_words_affect=bag_of_words_affect,\n",
    "        discrim=discrim,\n",
    "        class_label=class_label,\n",
    "        length=length,\n",
    "        stepsize=stepsize,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        sample=sample,\n",
    "        num_iterations=num_iterations,\n",
    "        grad_length=grad_length,\n",
    "        horizon_length=horizon_length,\n",
    "        window_length=window_length,\n",
    "        decay=decay,\n",
    "        gamma=gamma,\n",
    "        gm_scale=gm_scale,\n",
    "        kl_scale=kl_scale,\n",
    "        verbosity_level=verbosity_level\n",
    "    )\n",
    "\n",
    "    # untokenize unperturbed text\n",
    "    unpert_gen_text = tokenizer.decode(unpert_gen_tok_text.tolist()[0])\n",
    "\n",
    "    if verbosity_level >= REGULAR:\n",
    "        print(\"=\" * 80)\n",
    "    print(\"= Unperturbed generated text =\")\n",
    "    print(unpert_gen_text)\n",
    "    print()\n",
    "\n",
    "    generated_texts = []\n",
    "\n",
    "    # iterate through the perturbed texts\n",
    "    for i, pert_gen_tok_text in enumerate(pert_gen_tok_texts):\n",
    "        try:\n",
    "            # untokenize unperturbed text\n",
    "            pert_gen_text = tokenizer.decode(pert_gen_tok_text.tolist()[0])\n",
    "            print(\"= Perturbed generated text {} =\".format(i + 1))\n",
    "            print(pert_gen_text)\n",
    "            print()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # keep the prefix, perturbed seq, original seq for each index\n",
    "        generated_texts.append(\n",
    "            (tokenized_cond_text, pert_gen_tok_text, unpert_gen_tok_text)\n",
    "        )\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "iIdQW59daqIs"
   },
   "outputs": [],
   "source": [
    "def generate_text_pplm(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        affect_weight=0.2,\n",
    "        context=None,\n",
    "        past=None,\n",
    "        device=\"cuda\",\n",
    "        perturb=True,\n",
    "        bow_indices=None,\n",
    "        bow_indices_affect=None,\n",
    "        affect_int = None,\n",
    "        knob = None,\n",
    "        classifier=None,\n",
    "        class_label=None,\n",
    "        loss_type=0,\n",
    "        length=100,\n",
    "        stepsize=0.02,\n",
    "        temperature=1.0,\n",
    "        top_k=10,\n",
    "        sample=True,\n",
    "        num_iterations=3,\n",
    "        grad_length=10000,\n",
    "        horizon_length=1,\n",
    "        window_length=0,\n",
    "        decay=False,\n",
    "        gamma=1.5,\n",
    "        gm_scale=0.9,\n",
    "        kl_scale=0.01,\n",
    "        verbosity_level=REGULAR\n",
    "):\n",
    "    output_so_far = None\n",
    "    if context:\n",
    "        context_t = torch.tensor(context, device=device, dtype=torch.long)\n",
    "        while len(context_t.shape) < 2:\n",
    "            context_t = context_t.unsqueeze(0)\n",
    "        output_so_far = context_t\n",
    "\n",
    "    # collect one hot vectors for bags of words\n",
    "    one_hot_bows_vectors = build_bows_one_hot_vectors(bow_indices, tokenizer, device)\n",
    "    affect_int_orig = affect_int\n",
    "    one_hot_bows_affect, affect_int = build_bows_one_hot_vectors_aff(bow_indices_affect, affect_int, tokenizer, device)\n",
    "#    print(torch.FloatTensor(one_hot_bows_affect).size())\n",
    "    grad_norms = None\n",
    "    last = None\n",
    "    unpert_discrim_loss = 0\n",
    "    loss_in_time = []\n",
    "\n",
    "    if verbosity_level >= VERBOSE:\n",
    "        range_func = trange(length, ascii=True)\n",
    "    else:\n",
    "        range_func = range(length)\n",
    "    count = 0\n",
    "    int_score = 0\n",
    "    for i in range_func:\n",
    "        if count == 3:\n",
    "          break\n",
    "        # Get past/probs for current output, except for last word\n",
    "        # Note that GPT takes 2 inputs: past + current_token\n",
    "\n",
    "        # run model forward to obtain unperturbed\n",
    "        if past is None and output_so_far is not None:\n",
    "            last = output_so_far[:, -1:]\n",
    "            if output_so_far.shape[1] > 1:\n",
    "                _, past, _ = model(output_so_far[:, :-1])\n",
    "\n",
    "        unpert_logits, unpert_past, unpert_all_hidden = model(output_so_far)\n",
    "        unpert_last_hidden = unpert_all_hidden[-1]\n",
    "\n",
    "        # check if we are abowe grad max length\n",
    "        if i >= grad_length:\n",
    "            current_stepsize = stepsize * 0\n",
    "        else:\n",
    "            current_stepsize = stepsize\n",
    "\n",
    "        # modify the past if necessary\n",
    "        if not perturb or num_iterations == 0:\n",
    "            pert_past = past\n",
    "\n",
    "        else:\n",
    "            accumulated_hidden = unpert_last_hidden[:, :-1, :]\n",
    "            accumulated_hidden = torch.sum(accumulated_hidden, dim=1)\n",
    "\n",
    "            if past is not None:\n",
    "                pert_past, _, grad_norms, loss_this_iter = perturb_past(\n",
    "                    past,\n",
    "                    model,\n",
    "                    last,\n",
    "                    affect_weight = affect_weight,\n",
    "                    unpert_past=unpert_past,\n",
    "                    unpert_logits=unpert_logits,\n",
    "                    accumulated_hidden=accumulated_hidden,\n",
    "                    grad_norms=grad_norms,\n",
    "                    stepsize=current_stepsize,\n",
    "                    one_hot_bows_vectors=one_hot_bows_vectors,\n",
    "                    one_hot_bows_affect=one_hot_bows_affect,\n",
    "                    affect_int = affect_int,\n",
    "                    knob = knob,\n",
    "                    classifier=classifier,\n",
    "                    class_label=class_label,\n",
    "                    loss_type=loss_type,\n",
    "                    num_iterations=num_iterations,\n",
    "                    horizon_length=horizon_length,\n",
    "                    window_length=window_length,\n",
    "                    decay=decay,\n",
    "                    gamma=gamma,\n",
    "                    kl_scale=kl_scale,\n",
    "                    device=device,\n",
    "                    verbosity_level=verbosity_level\n",
    "                )\n",
    "                loss_in_time.append(loss_this_iter)\n",
    "            else:\n",
    "                pert_past = past\n",
    "\n",
    "        pert_logits, past, pert_all_hidden = model(last, past_key_values=pert_past)\n",
    "        pert_logits = pert_logits[:, -1, :] / temperature  # + SMALL_CONST\n",
    "        pert_probs = F.softmax(pert_logits, dim=-1)\n",
    "\n",
    "        if classifier is not None:\n",
    "            ce_loss = torch.nn.CrossEntropyLoss()\n",
    "            prediction = classifier(torch.mean(unpert_last_hidden, dim=1))\n",
    "            label = torch.tensor([class_label], device=device,\n",
    "                                 dtype=torch.long)\n",
    "            unpert_discrim_loss = ce_loss(prediction, label)\n",
    "            if verbosity_level >= VERBOSE:\n",
    "                print(\n",
    "                    \"unperturbed discrim loss\",\n",
    "                    unpert_discrim_loss.data.cpu().numpy()\n",
    "                )\n",
    "        else:\n",
    "            unpert_discrim_loss = 0\n",
    "\n",
    "        # Fuse the modified model and original model\n",
    "        if perturb:\n",
    "\n",
    "            unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\n",
    "\n",
    "            pert_probs = ((pert_probs ** gm_scale) * (\n",
    "                    unpert_probs ** (1 - gm_scale)))  # + SMALL_CONST\n",
    "            pert_probs = top_k_filter(pert_probs, k=top_k,\n",
    "                                      probs=True)  # + SMALL_CONST\n",
    "\n",
    "            # rescale\n",
    "            if torch.sum(pert_probs) <= 1:\n",
    "                pert_probs = pert_probs / torch.sum(pert_probs)\n",
    "\n",
    "        else:\n",
    "            pert_logits = top_k_filter(pert_logits, k=top_k)  # + SMALL_CONST\n",
    "            pert_probs = F.softmax(pert_logits, dim=-1)\n",
    "\n",
    "        # sample or greedy\n",
    "        if sample:\n",
    "            last = torch.multinomial(pert_probs, num_samples=1)\n",
    "            # print('pert_prob, last ', pert_probs, last)\n",
    "\n",
    "        else:\n",
    "            _, last = torch.topk(pert_probs, k=1, dim=-1)\n",
    "\n",
    "        # update context/output_so_far appending the new token\n",
    "        output_so_far = (\n",
    "            last if output_so_far is None\n",
    "            else torch.cat((output_so_far, last), dim=1)\n",
    "        )\n",
    "        if verbosity_level >= REGULAR:\n",
    "            print(tokenizer.decode(output_so_far.tolist()[0]))\n",
    "        if(tokenizer.decode(output_so_far.tolist()[0])[-1] == '.' ):\n",
    "          count = count+1\n",
    "        if bow_indices_affect is not None and [output_so_far.tolist()[0][-1]] in bow_indices_affect[0]:\n",
    "          int_word = affect_int_orig[bow_indices_affect[0].index([output_so_far.tolist()[0][-1]])]\n",
    "          print(tokenizer.decode(output_so_far.tolist()[0][-1]), int_word)\n",
    "          int_score = int_score + int_word\n",
    "    print(\"int_score: \", int_score)\n",
    "    # print(\"int.. \" , output_so_far.tolist()[0][-1])\n",
    "    return output_so_far, unpert_discrim_loss, loss_in_time\n",
    "\n",
    "\n",
    "def set_generic_model_params(discrim_weights, discrim_meta):\n",
    "    if discrim_weights is None:\n",
    "        raise ValueError('When using a generic discriminator, '\n",
    "                         'discrim_weights need to be specified')\n",
    "    if discrim_meta is None:\n",
    "        raise ValueError('When using a generic discriminator, '\n",
    "                         'discrim_meta need to be specified')\n",
    "\n",
    "    with open(discrim_meta, 'r') as discrim_meta_file:\n",
    "        meta = json.load(discrim_meta_file)\n",
    "    meta['path'] = discrim_weights\n",
    "    DISCRIMINATOR_MODELS_PARAMS['generic'] = meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "_mcJUcOpZB80"
   },
   "outputs": [],
   "source": [
    "# set the device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\" #if torch.cuda.is_available() and not no_cuda else \"cpu\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    " # load pretrained model\n",
    "pretrained_model=\"gpt2-medium\"\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "    pretrained_model,\n",
    "    output_hidden_states=True\n",
    ")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eNc2QcfQfaj"
   },
   "source": [
    "# Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "T_fvKlS8eFzL"
   },
   "outputs": [],
   "source": [
    "def f(Knob, Prompt, Topic, Affect):\n",
    "    run_pplm_example(\n",
    "          affect_weight=1,  # it is the convergence rate of affect loss, don't change it :-p\n",
    "          knob = Knob, # 0-1, play with it as much as you want\n",
    "          cond_text=Prompt,\n",
    "          num_samples=1,\n",
    "          bag_of_words=Topic,\n",
    "          bag_of_words_affect=Affect,\n",
    "          length=500,\n",
    "          stepsize=0.01,\n",
    "          sample=True,\n",
    "          num_iterations=3,\n",
    "          window_length=5,\n",
    "          gamma=1.5,\n",
    "          gm_scale=0.95,\n",
    "          kl_scale=0.01,\n",
    "          verbosity='quiet'\n",
    "      )\n",
    "def run():\n",
    "    interact_manual(f, Knob=(0,1, 0.1), Prompt=\"There exists\", \\\n",
    "         Topic = ['legal','military','monsters','politics','positive_words', 'religion', 'science','space','technology'], \\\n",
    "         Affect = ['fear', 'joy', 'anger', 'sadness', 'anticipation', 'disgust', 'surprise', 'trust']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "0439cb92092049b4a14c3cf1b636b5c4",
      "167e4351bff74792bea317b34d0156c1",
      "219fb0854b5c4dd195f3630fe7ce607a",
      "47b6d4a4d95a4854887aa45e9446625c",
      "9c952f336a3045da9ce304c8da4f81c5",
      "ea6f75d7dae64f1f88f64ee7a2a75595",
      "5f3865f5942e4f4596059a684566d280",
      "881f6e8cf2cb4518a02e4fe3ae96f260",
      "237ea3c12e9f43489f6b3279f8e0b8a0",
      "ddb813c3880c41ca8db7dc5acf60e696",
      "aba9e6e67ab5489abf867c1ebe855250",
      "1251f7681af64d1695c4fff67f548863",
      "eb5d3837f49045fa809bdfb7fe7236c1",
      "c350528a586b4720820c6b77aa250470",
      "31e5dd27337740148248d2b82a49377a",
      "c7a1f3aeb10949c9899ff47b59cfd1b9",
      "869e4090b8eb4036a4dbb4acc8606ee4",
      "13ded88e4bce422b99a213975c22a679",
      "db3b0ac319c940119df64fdeae233b5e"
     ]
    },
    "id": "nt3kOp5KLGk_",
    "outputId": "bcdab53f-34f4-4feb-b175-9cbc73d6080c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e870812b682459ca7ad0b012548b7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='Knob', max=1.0), Text(value='There exists', descriptâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "FlzDWG4y-HRH",
    "outputId": "6b288a68-a32b-48e5-ccc8-5251aaeb625b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: legal , affect: fear , knob is: 0.8\n",
      "= Prefix of sentence =\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py:746: FutureWarning: The `past` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int_score:  0\n",
      "After Perturbation\n",
      " shooting 0.812\n",
      " rampage 0.781\n",
      " killing 0.906\n",
      " violence 0.812\n",
      " shot 0.734\n",
      "int_score:  4.045\n",
      "= Unperturbed generated text =\n",
      "<|endoftext|>It is no secret that the United States is the most economically powerful nation in the world. However, it's important to realize how much more powerful we are when we consider our economic growth.\n",
      "\n",
      "The United States has a population that is now greater than the\n",
      "\n",
      "= Perturbed generated text 1 =\n",
      "<|endoftext|>It is the first time in US history that two women from opposing religious sects have been killed in a US shooting rampage.\n",
      "\n",
      "The killing in San Bernardino, California, of 14 people was sparked by a domestic violence dispute between the couple, who were shot dead\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics = ['legal']#,'military','monsters','politics','positive_words', 'religion', 'science','space','technology']\n",
    "affects = ['fear']#, 'anger', 'sadness'] #'fear', \n",
    "knob_vals = [0.8]#,0.5,0.7,1]\n",
    "\n",
    "for topic in topics:\n",
    "  for affect in affects:\n",
    "    for knob in knob_vals:\n",
    "      print(\"topic:\", topic, \", affect:\", affect, \", knob is:\", knob)\n",
    "      run_pplm_example(\n",
    "          affect_weight=1,  # it is the convergence rate of affect loss, don't change it :-p\n",
    "          knob = knob, # 0-1, play with it as much as you want\n",
    "          cond_text=\"It is\",\n",
    "          num_samples=1,\n",
    "          bag_of_words=topic,\n",
    "          bag_of_words_affect=affect,\n",
    "          length=50,\n",
    "          stepsize=0.005, #topic, affect convergence rate\n",
    "          sample=True,\n",
    "          num_iterations=10,\n",
    "          window_length=5,\n",
    "          gamma=1.5,\n",
    "          gm_scale=0.95,\n",
    "          kl_scale=0.01,\n",
    "          verbosity='quiet'\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54ZAm_bsTMI8"
   },
   "source": [
    "# Evaluate Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dysnnWMLAoEw"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch_pretrained_bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSUvDkuMAYP1"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTModel, OpenAIGPTLMHeadModel\n",
    "# Load pre-trained model (weights)\n",
    "model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt')\n",
    "model.eval()\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "\n",
    "\n",
    "# 21.31652459381952, 61.45907380241148, 26.24923942649312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHkf64i7ikjl"
   },
   "outputs": [],
   "source": [
    "def score(sentence):\n",
    "    tokenize_input = tokenizer.tokenize(sentence)\n",
    "    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "    loss=model(tensor_input, lm_labels=tensor_input)\n",
    "    # print(sentence, tokenize_input, tensor_input, loss)\n",
    "    return math.exp(loss)\n",
    "    \n",
    "# use your sentences that you want to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_9XprH-UBBu-"
   },
   "outputs": [],
   "source": [
    "a=['i am a girl . you are a boy .',\n",
    "                'there is a plane on the desk',\n",
    "                        \"there 's a pen on the desk\", \"this is me <eos>\", \"hi i want... i want your book\", \"there is is some noise\"]\n",
    "print([score(i) for i in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDGq-DLdBHjz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = []\n",
    "file_object = open('AffectLM_GenData.txt', 'r')\n",
    "for line in file_object:\n",
    "  data.append(line[:-1].replace('<eos>', '.').split(' #### '))\n",
    "  data[-1].append(score(data[-1][-1]))\n",
    "file_object.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-IkWREkduLf"
   },
   "outputs": [],
   "source": [
    "line = []\n",
    "with open('pplm.txt','r') as f:\n",
    "  for l in f.readlines():\n",
    "    l = l.split('\\t')\n",
    "    l[-1] = l[-1][:-1]\n",
    "    line.append(l)\n",
    "    \n",
    "line[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6sc-dClttEAn"
   },
   "outputs": [],
   "source": [
    "for i in range(len(line)):\n",
    "  line[i].append(score(line[i][2]))\n",
    "line[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRDNXNPZsv1w"
   },
   "outputs": [],
   "source": [
    "inten = [[] for i in range(6)]\n",
    "for i in line:\n",
    "  # if i[0] not in ['The book', 'The robots','The country', 'The issue focused on', 'The relationship','The road']:# and i[1] in task1emos:\n",
    "  if i[0] not in emos:\n",
    "    emos.append(i[1])\n",
    "  if i[1] =='0.01':\n",
    "    inten[0].append(i[3])\n",
    "  if i[1] =='0.02':\n",
    "    inten[1].append(i[3])\n",
    "  if i[1] =='0.05':\n",
    "    inten[2].append(i[3])\n",
    "  if i[1] =='0.1':\n",
    "    inten[3].append(i[3])\n",
    "  if i[1] =='0.5':\n",
    "    inten[4].append(i[3])\n",
    "  if i[1] =='1':\n",
    "    inten[5].append(i[3])\n",
    "\n",
    "\n",
    "[[inten.index(i),sum(i)/len(i)] for i in inten]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMlcqInDfknR"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('affLMdata_eval', 'wb') as f:\n",
    "  pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6oIiOjofrH3"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('affLMdata_eval', 'rb') as f:\n",
    "  data = pickle.load(f)\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiC44n9mv1SG"
   },
   "outputs": [],
   "source": [
    "data[10:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZzT936UjIRP"
   },
   "outputs": [],
   "source": [
    "inten = [[] for i in range(6)]\n",
    "task1emos = ['joy', 'sadness', 'anger', 'posemo', 'sad', 'angry']\n",
    "prompts = []\n",
    "emo = []\n",
    "for i in data:\n",
    "  if i[0] not in ['The book', 'The robots','The country', 'The issue focused on', 'The relationship','The road', 'the book', 'the robots','the country', 'the issue focused on', 'the relationship','the road'] and i[1] in task1emos:\n",
    "    if i[0] not in prompts:\n",
    "      prompts.append(i[0])\n",
    "    if i[1] not in emo:\n",
    "      emo.append(i[1])\n",
    "    if i[2] =='0.0':\n",
    "      inten[0].append(i[4])\n",
    "    if i[2] =='1.0':\n",
    "      inten[1].append(i[4])\n",
    "    if i[2] =='2.0':\n",
    "      inten[2].append(i[4])\n",
    "    if i[2] =='3.0':\n",
    "      inten[3].append(i[4])\n",
    "    if i[2] =='4.0':\n",
    "      inten[4].append(i[4])\n",
    "    if i[2] =='5.0':\n",
    "      inten[5].append(i[4])\n",
    "\n",
    "\n",
    "[[inten.index(i),sum(i)/len(i)] for i in inten], emo, prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuVEC1TujGBo"
   },
   "outputs": [],
   "source": [
    "pr = []\n",
    "for i in data:\n",
    "  if i[0] not in pr:\n",
    "    pr.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGPzMK6siUm4"
   },
   "outputs": [],
   "source": [
    "[[] for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpStDMNAaMg4"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('pplm_sentences_scores.jpg', 'rb') as f:\n",
    "  data = pickle.load(f)\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-ohblimabds"
   },
   "outputs": [],
   "source": [
    "# !touch pplm.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaVxnsFxUy3K"
   },
   "source": [
    "# Other Plottings and p-value Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3VBpE3cHbdvR"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uV-OvCCubnlj"
   },
   "outputs": [],
   "source": [
    "%cd /content/gdrive/My\\ Drive/NLP\\ |\\ AffTextGen\\ |\\ Human Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IReMECDvcAQM"
   },
   "outputs": [],
   "source": [
    "def get_affect_words_and_int (affect_class):\n",
    "  emotions = \"https://raw.githubusercontent.com/ishikasingh/Affective-text-gen/master/NRC-Emotion-Intensity-Lexicon-v1.txt\"\n",
    "  filepath = cached_path(emotions)\n",
    "  with open(filepath, \"r\") as f:\n",
    "      words = f.read().strip().split(\"\\n\")[1:]\n",
    "  words = [w.split(\"\\t\") for w in words]\n",
    "  return [w[0] for w in words if w[1] == affect_class], [float(w[-1]) for w in words if w[1] == affect_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYlGvPKF2K6t"
   },
   "outputs": [],
   "source": [
    "joy = get_affect_words_and_int('joy')\n",
    "anger = get_affect_words_and_int('anger')\n",
    "sad = get_affect_words_and_int('sadness')\n",
    "fear = get_affect_words_and_int('fear')\n",
    "anticipation = get_affect_words_and_int('anticipation')\n",
    "disgust = get_affect_words_and_int('disgust')\n",
    "surprise = get_affect_words_and_int('surprise')\n",
    "trust = get_affect_words_and_int('trust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1m2Ha6Bv3KOp"
   },
   "outputs": [],
   "source": [
    "i=11\n",
    "j = [joy[1][i] for i in range(len(joy[0])) if i%100==0]\n",
    "j1 = [joy[0][i] for i in range(len(joy[0])) if i%100==0]\n",
    "a = [anger[1][i] for i in range(len(anger[0])) if i%(int(len(anger[0])/12))==0]\n",
    "a1 = [anger[0][i] for i in range(len(anger[0])) if i%(int(len(anger[0])/12))==0]\n",
    "s = [sad[1][i] for i in range(len(sad[0])) if i%(int(len(sad[0])/12))==0]\n",
    "s1 = [sad[0][i] for i in range(len(sad[0])) if i%(int(len(sad[0])/12))==0]\n",
    "f = [fear[1][i] for i in range(len(fear[0])) if i%(int(len(fear[0])/12))==0]\n",
    "f1 = [fear[0][i] for i in range(len(fear[0])) if i%(int(len(fear[0])/12))==0]\n",
    "an = [anticipation[1][i] for i in range(len(anticipation[0])) if i%(int(len(anticipation[0])/12))==0]\n",
    "an1 = [anticipation[0][i] for i in range(len(anticipation[0])) if i%(int(len(anticipation[0])/12))==0]\n",
    "d = [disgust[1][i] for i in range(len(disgust[0])) if i%(int(len(disgust[0])/12))==0 and disgust[0][i]!='pornography']\n",
    "d1 = [disgust[0][i] for i in range(len(disgust[0])) if i%(int(len(disgust[0])/12))==0 and disgust[0][i]!='pornography']\n",
    "su = [surprise[1][i] for i in range(len(surprise[0])) if i%(int(len(surprise[0])/12))==0]\n",
    "su1 = [surprise[0][i] for i in range(len(surprise[0])) if i%(int(len(surprise[0])/12))==0]\n",
    "t = [trust[1][i] for i in range(len(trust[0])) if i%(int(len(trust[0])/12))==0]\n",
    "t1 = [trust[0][i] for i in range(len(trust[0])) if i%(int(len(trust[0])/12))==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ga5EAQ8Q3O2g"
   },
   "outputs": [],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNxTeMhk4DS4"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(15, 7))\n",
    "# fig.suptitle('bold figure suptitle', fontsize=14, fontweight='bold')\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "fig.subplots_adjust(top=0.99)\n",
    "# fig.subplots_adjust(bottom=-0.9)\n",
    "# ax.set_title('axes title')\n",
    "\n",
    "ax.set_xlabel('Emotion Category',fontsize=12)\n",
    "ax.set_ylabel('Emotion Intensity', fontsize=12)\n",
    "\n",
    "# ax.text(3, 8, 'boxed italics text in data coords', style='italic',\n",
    "        # bbox={'facecolor': 'red', 'alpha': 0.5, 'pad': 10})\n",
    "\n",
    "# ax.text(2, 6, r'an equation: $E=mc^2$', fontsize=15)\n",
    "\n",
    "# ax.text(0.95, 0.01, 'colored text in axes coords',\n",
    "#         verticalalignment='bottom', horizontalalignment='right',\n",
    "#         transform=ax.transAxes,\n",
    "#         color='green', fontsize=15)\n",
    "\n",
    "em=['joy']*len(j)+['fear']*len(f)+ ['anger']*len(a)+ ['sadness']*(len(s)-1)+ ['anticipation']*len(an)+ ['disgust']*len(d)+ ['surprise']*len(su)+ ['trust']*len(t)\n",
    "emnum = em=[0]*len(j)+[1]*len(f)+ [2]*len(a)+ [3]*(len(s)-1)+ [4]*len(an)+ [5]*len(d)+ [6]*len(su)+ [7]*len(t)\n",
    "inten = j+f+a+s[:-1]+an+d+su+t\n",
    "word = j1+f1+a1+s1[:-1]+an1+d1+su1+t1\n",
    "for i in range(len(em)):\n",
    "  ax.text(emnum[i]+0.04, inten[i]-0.004, word[i], fontsize=11)\n",
    "\n",
    "ax.plot(['joy']*len(j) ,j , 'o',color= '#33A7FF')#0000FF)\n",
    "ax.plot(['fear']*len(f) ,f, 'o',color= '#990012')\n",
    "ax.plot(['anger']*len(a) ,a, 'o',color= '#E42217')\n",
    "ax.plot(['sadness']*(len(s)-1) ,s[:-1], 'o',color= '#151B8D')\n",
    "ax.plot(['anticipation']*len(an) ,an, 'o',color= '#FF8040')\n",
    "ax.plot(['disgust']*len(d) ,d, 'o',color= '#8E35EF')\n",
    "ax.plot(['surprise']*len(su) ,su, 'o',color= '#FDD017')\n",
    "ax.plot(['trust']*len(t) ,t, 'o',color= '#4CC417')\n",
    "plt.xticks(fontsize=12)\n",
    "ax.axis([-1, 8, 0, 1])\n",
    "plt.savefig('emotion-word-dist.pdf', format='pdf', dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZmdx4hByr3p"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfECnK_6zK1g"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "import gspread\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "gc = gspread.authorize(GoogleCredentials.get_application_default())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zaWqlYva4Guw"
   },
   "outputs": [],
   "source": [
    "worksheet = gc.open('Human_Eval_Reponses').sheet1\n",
    "\n",
    "# get_all_values gives a list of rows.\n",
    "rows = worksheet.get_all_values()\n",
    "print(rows)\n",
    "\n",
    "import pandas as pd\n",
    "task1 = pd.DataFrame.from_records(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PevjyF7K1KCP"
   },
   "outputs": [],
   "source": [
    "task1_aff = task1[:36]\n",
    "task1_our = task1.iloc[36:]\n",
    "task1_aff.iloc[-1], task1_our.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwgFcr0h29s4"
   },
   "outputs": [],
   "source": [
    "task1_aff.iloc[10][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6chMMctY4CZR"
   },
   "outputs": [],
   "source": [
    "len(task1_our)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zUEPreovyp2D"
   },
   "outputs": [],
   "source": [
    "for j in range(3,8):\n",
    "  a1=0; a2 =0; a3 = 0; a1c=0; a2c=0; a3c=0\n",
    "  for i in range(len(task1_aff)):\n",
    "    if task1_aff.iloc[i][2]=='1':\n",
    "      a1+=1\n",
    "      if task1_aff.iloc[i][1] == task1_aff.iloc[i][j]:\n",
    "        a1c+=1\n",
    "    if task1_aff.iloc[i][2]=='2':\n",
    "      a2+=1\n",
    "      if task1_aff.iloc[i][1] == task1_aff.iloc[i][j]:\n",
    "        a2c+=1\n",
    "    if task1_aff.iloc[i][2]=='3':\n",
    "      a3+=1\n",
    "      if task1_aff.iloc[i][1] == task1_aff.iloc[i][j]:\n",
    "        a3c+=1\n",
    "  print(a1c/a1, a2c/a2, a3c/a3, a1c,a1, a2c,a2, a3c,a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ZPoK4rD0eA8"
   },
   "outputs": [],
   "source": [
    "for j in range(3,8):\n",
    "  a1=0; a2 =0; a3 = 0; a1c=0; a2c=0; a3c=0\n",
    "  for i in range(len(task1_our)):\n",
    "    if task1_our.iloc[i][2]=='0.4':\n",
    "      a1+=1\n",
    "      if task1_our.iloc[i][1] == task1_our.iloc[i][j]:\n",
    "        a1c+=1\n",
    "    if task1_our.iloc[i][2]=='0.6':\n",
    "      a2+=1\n",
    "      if task1_our.iloc[i][1] == task1_our.iloc[i][j]:\n",
    "        a2c+=1\n",
    "    if task1_our.iloc[i][2]=='1':\n",
    "      a3+=1\n",
    "      if task1_our.iloc[i][1] == task1_our.iloc[i][j]:\n",
    "        a3c+=1\n",
    "  print(a1c/a1, a2c/a2, a3c/a3, a1c,a1, a2c,a2, a3c,a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kphjxrlz-bbN"
   },
   "outputs": [],
   "source": [
    "task2[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hsR-MyLN6Ghv"
   },
   "outputs": [],
   "source": [
    "for j in range(2,6):\n",
    "  a1=0; a2 =0; a3 = 0; a1c=0; a2c=0; a3c=0\n",
    "  for i in range(6, len(task2)):\n",
    "    # print(task2.iloc[i])\n",
    "    if task2.iloc[i][1]=='0.4':\n",
    "      a1+=1\n",
    "      if task2.iloc[i][0] == task2.iloc[i][j]:\n",
    "        a1c+=1\n",
    "    if task2.iloc[i][1]=='0.6':\n",
    "      a2+=1\n",
    "      if task2.iloc[i][0] == task2.iloc[i][j]:\n",
    "        a2c+=1\n",
    "    if task2.iloc[i][1]=='1':\n",
    "      a3+=1\n",
    "      if task2.iloc[i][0] == task2.iloc[i][j]:\n",
    "        a3c+=1\n",
    "  print(a1c/a1, a2c/a2, a3c/a3, a1c,a1, a2c,a2, a3c,a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXelDh24fT9n"
   },
   "outputs": [],
   "source": [
    "!pip install pingouin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cy2NCb4AffA_"
   },
   "outputs": [],
   "source": [
    "worksheet = gc.open('Human_Eval_Reponses').sheet1\n",
    "\n",
    "# get_all_values gives a list of rows.\n",
    "rows = worksheet.get_all_values()\n",
    "print(rows)\n",
    "\n",
    "import pandas as pd\n",
    "task3 = pd.DataFrame.from_records(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYjpMYOspQAH"
   },
   "outputs": [],
   "source": [
    "task3[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSDvwtJUoHRN"
   },
   "outputs": [],
   "source": [
    "act_int = list(task3[6:6+234][2])\n",
    "avg = list(task3[6:][8]) ## aditya's-7\n",
    "print(len(act_int), len(avg))\n",
    "i=0; gram=[]; pred_int=[]\n",
    "while i<len(avg):\n",
    "  gram+=avg[i:i+6]\n",
    "  pred_int+=avg[i+6:i+12]\n",
    "  i+=12\n",
    "len(gram), len(pred_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sp_aQ7jAqyVR"
   },
   "outputs": [],
   "source": [
    "act_int = [float(i) for i in act_int]\n",
    "gram = [float(i) for i in gram]\n",
    "pred_int = [float(i)+1 for i in pred_int]\n",
    "act_int[:2], gram[:7], pred_int[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKSXFKkK6-SV"
   },
   "outputs": [],
   "source": [
    "([0.0, 0.6],\n",
    " [5.25, 4.5, 4.0, 6.75, 5.75, 6.75, 6.0],\n",
    " [3.0, 3.5, 2.75, 3.5, 2.0, 1.25, 1.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIMxhjOuszHH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_C9Cp-yrcsJ"
   },
   "outputs": [],
   "source": [
    "i=0; g= [[] for i in range(13)]; p = [[] for i in range(13)]; j=0\n",
    "while i <len(act_int):\n",
    "  data = pd.DataFrame(list(zip(act_int[i:i+18], gram[i:i+18], pred_int[i:i+18])), \n",
    "                columns =[ 'act', 'gram', 'pred_int']) \n",
    "  \n",
    "  data = data.sort_values('act')\n",
    "  g[j].append([np.mean(list(data['gram'])[3*i:3*i+3]) for i in range(6)])\n",
    "  p[j].append([np.mean(list(data['pred_int'])[3*i:3*i+3]) for i in range(6)])\n",
    "  # print('gr', g)\n",
    "  # print('pi', p)\n",
    "  j+=1\n",
    "  i+=18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFymgsQ0rLMT"
   },
   "outputs": [],
   "source": [
    "g[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5F9pKHHbHn0"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ax.plot([1, 2])\n",
    "# ax.set_xlabel('x-label', fontsize=12)\n",
    "# ax.set_ylabel('y-label', fontsize=12)\n",
    "# ax.set_title('Title', fontsize=14)\n",
    "\n",
    "fig2 = plt.figure(constrained_layout=True, figsize=(15, 7))\n",
    "spec2 = gridspec.GridSpec(ncols=3, nrows=2, figure=fig2)\n",
    "\n",
    "f2_ax1 = fig2.add_subplot(spec2[0, 0])\n",
    "f2_ax1.set_xticks([])\n",
    "f2_ax1.set_title('Our Model', fontsize=14)\n",
    "f2_ax1.set_ylabel('Perplexity\\n (Automated Evaluation)\\n (lower is better)', fontsize=12)\n",
    "perp = [32.92,29.19 ,33.49,30.91,29.37,30.85]\n",
    "int_ = [0.0,0.2,0.4,0.6,0.8,1.0]\n",
    "f2_ax1.plot(int_, perp,'o-', label = 'Avg. for 3 emotions')\n",
    "perp= [28.75,29.43,30.09,31.56,30.84,28.58]\n",
    "f2_ax1.plot(int_, perp,'*-', label = 'Avg. for 8 emotions')\n",
    "f2_ax1.legend()\n",
    "f2_ax1.axis([0, 1, 10, 65])\n",
    "f2_ax1.set_xlabel('(a)', fontsize = 12)\n",
    "\n",
    "\n",
    "f2_ax2 = fig2.add_subplot(spec2[0, 1])\n",
    "f2_ax2.set_xticks([])\n",
    "f2_ax2.set_yticks([])\n",
    "f2_ax2.set_title('Affect LM', fontsize=14)\n",
    "perp = [41.01, 36.77,41.84,43.63, 47.86,59.64]\n",
    "int_ = [0,1,2,3,4,5]\n",
    "f2_ax2.plot(int_, perp,'o-', label = 'Avg. for 3 emotions')\n",
    "f2_ax2.axis([0, 5, 10, 65])\n",
    "f2_ax2.legend()\n",
    "f2_ax2.set_xlabel('(b)', fontsize = 12)\n",
    "\n",
    "\n",
    "f2_ax5 = fig2.add_subplot(spec2[0, 2])\n",
    "f2_ax5.set_xticks([])\n",
    "f2_ax5.set_yticks([])\n",
    "perp = [14.38,13.13 ,11.95,15.96,64.47,58.76]\n",
    "int_ =[0.01,0.02,0.05,0.10,0.50,1.00]\n",
    "f2_ax5.plot(int_, perp,'o-', label = 'Avg. for 2 sentiments')\n",
    "f2_ax5.axis([0, 1, 10, 65])\n",
    "f2_ax5.legend()\n",
    "f2_ax5.set_title('PPLM', fontsize=14)\n",
    "f2_ax5.set_xlabel('(c)', fontsize = 12)\n",
    "\n",
    "f2_ax3 = fig2.add_subplot(spec2[1, 0])\n",
    "int_ = [0.0,0.2,0.4,0.6,0.8,1.0]\n",
    "f2_ax3.plot(int_, g[0][0],'o-', label = 'Anger')\n",
    "f2_ax3.plot(int_, g[1][0],'*-', label = 'Sadness')\n",
    "f2_ax3.plot(int_, g[2][0],'+-', label = 'Joy')\n",
    "f2_ax3.plot(int_, g[8][0],'x-', label = 'Fear')\n",
    "f2_ax3.plot(int_, g[9][0],'v-', label = 'Anticipation')\n",
    "f2_ax3.plot(int_, g[10][0],'D-', label = 'Disgust')\n",
    "f2_ax3.plot(int_, g[11][0],'d-', label = 'Surprise')\n",
    "f2_ax3.plot(int_, g[12][0],'^-', label = 'Trust')\n",
    "f2_ax3.legend()\n",
    "f2_ax3.set_ylabel('Grammatical Correctness\\n (Human Evaluation)\\n (higher is better)', fontsize=12)\n",
    "f2_ax3.axis([0, 1, 1, 7])\n",
    "f2_ax3.set_xlabel('(d)\\nEmotion Intensity (Knob)', fontsize = 12)\n",
    "\n",
    "f2_ax4 = fig2.add_subplot(spec2[1, 1])\n",
    "f2_ax4.set_yticks([])\n",
    "int_ = [0,1,2,3,4,5]\n",
    "f2_ax4.plot(int_, g[3][0],'o-', label = 'Anger')\n",
    "f2_ax4.plot(int_, g[4][0],'*-', label = 'Sadness')\n",
    "f2_ax4.plot(int_, g[5][0],'+-', label = 'Positive Emotion')\n",
    "f2_ax4.legend()\n",
    "f2_ax4.axis([0, 5, 1, 7])\n",
    "f2_ax4.set_xlabel('(e)\\nEmotion Intensity (beta)', fontsize = 12)\n",
    "\n",
    "f2_ax6 = fig2.add_subplot(spec2[1, 2])\n",
    "f2_ax6.set_yticks([])\n",
    "int_ =[0.01,0.02,0.05,0.10,0.50,1.00]\n",
    "f2_ax6.plot(int_, g[6][0],'o-', label = 'Negative Sentiment')\n",
    "f2_ax6.plot(int_, g[7][0],'*-', label = 'Positive Sentiment')\n",
    "f2_ax6.legend()\n",
    "f2_ax6.axis([0, 1, 1,7])\n",
    "f2_ax6.set_xlabel('(f)\\nSentiment Intensity (Stepsize)', fontsize = 12)\n",
    "fig2.savefig('grammer_2annot.svg', format='svg', dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8KX0l_ANZp4"
   },
   "outputs": [],
   "source": [
    "fig2 = plt.figure(constrained_layout=True, figsize=(15, 5))\n",
    "spec2 = gridspec.GridSpec(ncols=3, nrows=1, figure=fig2)\n",
    "\n",
    "# f2_ax1 = fig2.add_subplot(spec2[0, 0])\n",
    "# f2_ax1.set_xticks([])\n",
    "# f2_ax1.set_title('Our Model', fontsize=14)\n",
    "# f2_ax1.set_ylabel('Perplexity\\n (Automated Evaluation)\\n (lower is better)', fontsize=12)\n",
    "# perp = [32.92,29.19 ,33.49,30.91,29.37,30.85]\n",
    "# int_ = [0.0,0.2,0.4,0.6,0.8,1.0]\n",
    "# f2_ax1.plot(int_, perp,'o-', label = 'Avg. for 3 emotions')\n",
    "# perp= [28.75,29.43,30.09,31.56,30.84,28.58]\n",
    "# f2_ax1.plot(int_, perp,'*-', label = 'Avg. for 8 emotions')\n",
    "# f2_ax1.legend()\n",
    "# f2_ax1.axis([0, 1, 10, 65])\n",
    "# f2_ax1.set_xlabel('(a)', fontsize = 12)\n",
    "\n",
    "\n",
    "# f2_ax2 = fig2.add_subplot(spec2[0, 1])\n",
    "# f2_ax2.set_xticks([])\n",
    "# f2_ax2.set_yticks([])\n",
    "# f2_ax2.set_title('Affect LM', fontsize=14)\n",
    "# perp = [41.01, 36.77,41.84,43.63, 47.86,59.64]\n",
    "# int_ = [0,1,2,3,4,5]\n",
    "# f2_ax2.plot(int_, perp,'o-', label = 'Avg. for 3 emotions')\n",
    "# f2_ax2.axis([0, 5, 10, 65])\n",
    "# f2_ax2.legend()\n",
    "# f2_ax2.set_xlabel('(b)', fontsize = 12)\n",
    "\n",
    "\n",
    "# f2_ax5 = fig2.add_subplot(spec2[0, 2])\n",
    "# f2_ax5.set_xticks([])\n",
    "# f2_ax5.set_yticks([])\n",
    "# perp = [14.38,13.13 ,11.95,15.96,64.47,58.76]\n",
    "# int_ =[0.01,0.02,0.05,0.10,0.50,1.00]\n",
    "# f2_ax5.plot(int_, perp,'o-', label = 'Avg. for 2 sentiments')\n",
    "# f2_ax5.axis([0, 1, 10, 65])\n",
    "# f2_ax5.legend()\n",
    "# f2_ax5.set_title('PPLM', fontsize=14)\n",
    "# f2_ax5.set_xlabel('(c)', fontsize = 12)\n",
    "\n",
    "f2_ax3 = fig2.add_subplot(spec2[0, 0])\n",
    "int_ = [0.0,0.2,0.4,0.6,0.8,1.0]\n",
    "f2_ax3.plot(int_, p[0][0],'o-', label = 'Anger')\n",
    "f2_ax3.plot(int_, p[1][0],'*-', label = 'Sadness')\n",
    "f2_ax3.plot(int_, p[2][0],'+-', label = 'Joy')\n",
    "f2_ax3.plot(int_, p[8][0],'x-', label = 'Fear')\n",
    "f2_ax3.plot(int_, p[9][0],'v-', label = 'Anticipation')\n",
    "f2_ax3.plot(int_, p[10][0],'D-', label = 'Disgust')\n",
    "f2_ax3.plot(int_, p[11][0],'d-', label = 'Surprise')\n",
    "f2_ax3.plot(int_, p[12][0],'^-', label = 'Trust')\n",
    "avg8 = [sum(x)/8 for x in zip(p[0][0], p[1][0],p[2][0],p[8][0],p[9][0],p[10][0],p[11][0],p[12][0])]\n",
    "f2_ax3.plot(int_, avg8,'s-', label = 'Average of 8 emotions', color=\"black\")\n",
    "f2_ax3.legend()\n",
    "f2_ax3.set_title('Our Model', fontsize=14)\n",
    "f2_ax3.set_ylabel('Emotion Intensity Ranking\\n(Human Evaluation)', fontsize=12)\n",
    "f2_ax3.axis([0, 1, 1, 6])\n",
    "f2_ax3.set_xlabel('(a)\\nEmotion Intensity (Knob)', fontsize = 12)\n",
    "\n",
    "f2_ax4 = fig2.add_subplot(spec2[0, 1])\n",
    "f2_ax4.set_yticks([])\n",
    "int_ = [0,1,2,3,4,5]\n",
    "f2_ax4.plot(int_, p[3][0],'o-', label = 'Anger')\n",
    "f2_ax4.plot(int_, p[4][0],'*-', label = 'Sadness')\n",
    "f2_ax4.plot(int_, p[5][0],'+-', label = 'Positive Emotion')\n",
    "avg3 = [sum(x)/3 for x in zip(p[3][0], p[4][0],p[5][0])]\n",
    "f2_ax4.plot(int_, avg3,'s-', label = 'Average of 3 emotions', color=\"black\")\n",
    "f2_ax4.legend()\n",
    "f2_ax4.axis([0, 5, 1, 6])\n",
    "f2_ax4.set_title('Affect LM', fontsize=14)\n",
    "f2_ax4.set_xlabel('(b)\\nEmotion Intensity (beta)', fontsize = 12)\n",
    "\n",
    "f2_ax6 = fig2.add_subplot(spec2[0, 2])\n",
    "f2_ax6.set_yticks([])\n",
    "int_ =[0.01,0.02,0.05,0.10,0.50,1.00]\n",
    "f2_ax6.plot(int_, p[6][0],'o-', label = 'Negative Sentiment')\n",
    "f2_ax6.plot(int_, p[7][0],'*-', label = 'Positive Sentiment')\n",
    "avg2 = [sum(x)/2 for x in zip(p[6][0], p[7][0])]\n",
    "f2_ax6.plot(int_, avg2,'s-', label = 'Average of 2 sentiments', color=\"black\")\n",
    "f2_ax6.legend()\n",
    "f2_ax6.axis([0, 1, 1,6])\n",
    "f2_ax6.set_title('PPLM', fontsize=14)\n",
    "f2_ax6.set_xlabel('(c)\\nSentiment Intensity (Stepsize)', fontsize = 12)\n",
    "\n",
    "fig2.savefig('pred_intensity_2annot.svg', format='svg', dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRWlZbl5Ixq3"
   },
   "outputs": [],
   "source": [
    "fig2 = plt.figure(constrained_layout=True, figsize=(10, 5))\n",
    "spec2 = gridspec.GridSpec(ncols=2, nrows=1, figure=fig2)\n",
    "\n",
    "# f2_ax1 = fig2.add_subplot(spec2[0, 0])\n",
    "# f2_ax1.set_xticks([])\n",
    "# f2_ax1.set_title('Our Model', fontsize=14)\n",
    "# f2_ax1.set_ylabel('Perplexity\\n (Automated Evaluation)\\n (lower is better)', fontsize=12)\n",
    "# perp = [32.92,29.19 ,33.49,30.91,29.37,30.85]\n",
    "# int_ = [0.0,0.2,0.4,0.6,0.8,1.0]\n",
    "# f2_ax1.plot(int_, perp,'o-', label = 'Avg. for 3 emotions')\n",
    "# perp= [28.75,29.43,30.09,31.56,30.84,28.58]\n",
    "# f2_ax1.plot(int_, perp,'*-', label = 'Avg. for 8 emotions')\n",
    "# f2_ax1.legend()\n",
    "# f2_ax1.axis([0, 1, 10, 65])\n",
    "# f2_ax1.set_xlabel('(a)', fontsize = 12)\n",
    "\n",
    "\n",
    "# f2_ax2 = fig2.add_subplot(spec2[0, 1])\n",
    "# f2_ax2.set_xticks([])\n",
    "# f2_ax2.set_yticks([])\n",
    "# f2_ax2.set_title('Affect LM', fontsize=14)\n",
    "# perp = [41.01, 36.77,41.84,43.63, 47.86,59.64]\n",
    "# int_ = [0,1,2,3,4,5]\n",
    "# f2_ax2.plot(int_, perp,'o-', label = 'Avg. for 3 emotions')\n",
    "# f2_ax2.axis([0, 5, 10, 65])\n",
    "# f2_ax2.legend()\n",
    "# f2_ax2.set_xlabel('(b)', fontsize = 12)\n",
    "\n",
    "\n",
    "# f2_ax5 = fig2.add_subplot(spec2[0, 2])\n",
    "# f2_ax5.set_xticks([])\n",
    "# f2_ax5.set_yticks([])\n",
    "# perp = [14.38,13.13 ,11.95,15.96,64.47,58.76]\n",
    "# int_ =[0.01,0.02,0.05,0.10,0.50,1.00]\n",
    "# f2_ax5.plot(int_, perp,'o-', label = 'Avg. for 2 sentiments')\n",
    "# f2_ax5.axis([0, 1, 10, 65])\n",
    "# f2_ax5.legend()\n",
    "# f2_ax5.set_title('PPLM', fontsize=14)\n",
    "# f2_ax5.set_xlabel('(c)', fontsize = 12)\n",
    "\n",
    "f2_ax3 = fig2.add_subplot(spec2[0, 0])\n",
    "int_ = [0.0,0.2,0.4,0.6,0.8,1.0]\n",
    "Ang = [0.451, 0.465, 0.472, 0.473, 0.472, 0.463]\n",
    "joy = [0.403, 0.409, 0.405, 0.425, 0.426, 0.412]\n",
    "sad = [0.421, 0.428, 0.428, 0.433, 0.433, 0.420]\n",
    "f2_ax3.plot(int_, Ang,'o-', label = 'Anger')\n",
    "f2_ax3.plot(int_, sad,'*-', label = 'Sadness')\n",
    "f2_ax3.plot(int_, joy,'+-', label = 'Joy')\n",
    "f2_ax3.legend()\n",
    "f2_ax3.set_ylabel('Emotion Intensity Rating\\n(Automated Evaluation)', fontsize=12)\n",
    "f2_ax3.axis([0, 1, 0.4, 0.5])\n",
    "f2_ax3.set_title('Our Model', fontsize=14)\n",
    "f2_ax3.set_xlabel('(a)\\nEmotion Intensity (Knob)', fontsize = 12)\n",
    "\n",
    "f2_ax4 = fig2.add_subplot(spec2[0, 1])\n",
    "# f2_ax4.set_yticks([])\n",
    "int_ = [0,1,2,3,4,5]\n",
    "pos_em=[0.446, 0.458, 0.501, 0.572, 0.638, 0.621]\n",
    "sad= [0.403, 0.417, 0.419, 0.431, 0.435, 0.446]\n",
    "anger = [0.429, 0.435, 0.479, 0.525, 0.557, 0.579]\n",
    "f2_ax4.plot(int_, anger,'o-', label = 'Anger')\n",
    "f2_ax4.plot(int_, sad,'*-', label = 'Sadness')\n",
    "f2_ax4.plot(int_, pos_em,'+-', label = 'Positive Emotion')\n",
    "f2_ax4.legend()\n",
    "f2_ax4.set_title('Affect LM', fontsize=14)\n",
    "f2_ax4.axis([0, 5, 0.4,0.7])\n",
    "f2_ax4.set_xlabel('(b)\\nEmotion Intensity (beta)', fontsize = 12)\n",
    "\n",
    "# f2_ax6 = fig2.add_subplot(spec2[0, 2])\n",
    "# f2_ax6.set_yticks([])\n",
    "# int_ =[0.01,0.02,0.05,0.10,0.50,1.00]\n",
    "# f2_ax6.plot(int_, p[6][0],'o-', label = 'Negative Sentiment')\n",
    "# f2_ax6.plot(int_, p[7][0],'*-', label = 'Positive Sentiment')\n",
    "# f2_ax6.legend()\n",
    "# f2_ax6.axis([0, 1, 1,6])\n",
    "# f2_ax6.set_xlabel('(c)\\nSentiment Intensity (Stepsize)', fontsize = 12)\n",
    "fig2.savefig('pred_intensity_auto.svg', format='svg', dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QdQpdWn5i23K"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDpkJYgU-6vQ"
   },
   "outputs": [],
   "source": [
    "i=0; m = np.zeros(18); a =0\n",
    "while i <len(act_int):\n",
    "  \n",
    "  data = pd.DataFrame(list(zip(act_int[i:i+18], gram[i:i+18])), \n",
    "                columns =[ 'group', 'weight']) \n",
    "  # print(data)\n",
    "  # data = \"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/PlantGrowth.csv\"\n",
    "\n",
    "  # df = pd.read_csv(data, index_col=0)\n",
    "  # data = data.sort_values('group')\n",
    "  # we = [np.mean(list(data['weight'])[3*i:3*i+3]) for i in range(6)]\n",
    "  # gr = [np.mean(list(data['group'])[3*i:3*i+3]) for i in range(6)]\n",
    "  # data1 = pd.DataFrame(list(zip(gr, we)), \n",
    "  #               columns =[ 'group', 'weight'])\n",
    "  # if a in [0,1,2,8,9,10,11,12]:\n",
    "  #   print(a)\n",
    "  #   m+=np.array(data['weight'])\n",
    "  aov = pg.anova(data=data, dv='weight', between='group', detailed=True)\n",
    "  print(aov)\n",
    "  # pt = pg.pairwise_tukey(dv='weight', between='group', data=data)\n",
    "  # print(pt)\n",
    "  i+=18\n",
    "  a+=1\n",
    "data = pd.DataFrame(list(zip(act_int[:18], list(m))), \n",
    "                columns =[ 'group', 'weight'])\n",
    "aov = pg.anova(data=data, dv='weight', between='group', detailed=True)\n",
    "print(aov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOH_O4nti4vS"
   },
   "outputs": [],
   "source": [
    "0.317, 0.010, 0.048, 0.002, 0.000, 0.0167, 0.352, 0.415, 0.639, 0.203, 0.000, 0.188, 0.067\n",
    "0.451, 0.146, 0.330, 0.553, -, 1, 0.135, 0.008, 0.749, 0.758, 0.692, 0.625, 0.671"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hn8Ew_SxwA7R"
   },
   "outputs": [],
   "source": [
    "from statsmodels.multivariate.manova import MANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFE7zFRqfDS9"
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "while i <len(act_int):\n",
    "  data = pd.DataFrame(list(zip(act_int[i:i+18], gram[i:i+18], pred_int[i:i+18])), \n",
    "                columns =[ 'group', 'gram', 'pred_int']) \n",
    "  maov = MANOVA.from_formula('gram + pred_int ~ group', data=data)\n",
    "  print(maov.mv_test())\n",
    "  i+=18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wemKFfidv8r3"
   },
   "outputs": [],
   "source": [
    "!pip install krippendorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "voBiFtgcufvl"
   },
   "outputs": [],
   "source": [
    "import krippendorff\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ANCoI9yuD_s"
   },
   "outputs": [],
   "source": [
    "task3[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9kZ1FnZuGf9"
   },
   "outputs": [],
   "source": [
    "reldata1 = []\n",
    "reldata2 = []\n",
    "for j in range(4):\n",
    "  avg = list(task3[6:][j+4]) ## aditya's-7\n",
    "  print(len(avg))\n",
    "  i=0; gram=[]; pred_int=[]\n",
    "  while i<len(avg):\n",
    "    gram+=avg[i:i+6]\n",
    "    pred_int+=avg[i+6:i+12]\n",
    "    i+=12\n",
    "  gram = [float(i) for i in gram]\n",
    "  pred_int = [float(i)+1 for i in pred_int]\n",
    "  reldata1.append(pred_int)\n",
    "  reldata2.append(gram)\n",
    "print(len(reldata1))\n",
    "reliability_data = np.array(reldata1)\n",
    "reliability_data.reshape(4, 234)\n",
    "reliability_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PwWji5D86FF"
   },
   "outputs": [],
   "source": [
    "reliability_data = np.array(reldata2)\n",
    "reliability_data.reshape(4, 234)\n",
    "reliability_data.shape\n",
    "reliability_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXytzHVBzMKZ"
   },
   "outputs": [],
   "source": [
    "reldata = [list(reliability_data[1]), list(reliability_data[3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTCltF4r80TH"
   },
   "outputs": [],
   "source": [
    "krippendorff.alpha(reldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMmRz70S-BIK"
   },
   "outputs": [],
   "source": [
    "01 -0.09, 02 -0.13, 03 0.08\n",
    "12 0.47, 13 0.23\n",
    "23 0.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHmD5pw60yXh"
   },
   "outputs": [],
   "source": [
    "task1 =task1.replace(['Anger'], 0).replace(['Positive Emotion'], 1).replace(['Neutral'], 2).replace(['Sadness'], 3)\n",
    "task1[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzeRtmM5zvOW"
   },
   "outputs": [],
   "source": [
    "reldata = [list(task1[:72][7]), list(task1[:72][6])]#, list(task1[:72][6]), list(task1[:72][7])], 3,4,6,7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQGZhV7g28_g"
   },
   "outputs": [],
   "source": [
    "krippendorff.alpha(reldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKhygaN12g5U"
   },
   "outputs": [],
   "source": [
    "task2 =task2.replace(['Anger'], 0).replace(['Joy'], 1).replace(['Fear'], 2).replace(['Sadness'], 3).replace(['Disgust'], 4).replace(['Anticipation'], 5).replace(['Surprise'], 6).replace(['Trust'], 7)\n",
    "task2[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DnPD1Ein2995"
   },
   "outputs": [],
   "source": [
    "reldata = [list(task2[6:][4]), list(task2[6:][5])]#, list(task2[6:][4]), list(task2[6:][5])], 2,3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9OLwMCGtk-H"
   },
   "outputs": [],
   "source": [
    "krippendorff.alpha(reldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrEcEKiLubTR"
   },
   "outputs": [],
   "source": [
    "0.804, 0.162 | \n",
    "0.36, 43 0.33, 46 0.54, 47 0.29, 37 0.22, 36 0.41, 67 0.31\n",
    "0.28, 23 0.26, 24 0.40, 25 0.15, 34 0.15, 35 0.42, 45 0.30"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "AffectiveTextGen.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0439cb92092049b4a14c3cf1b636b5c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [
       "widget-interact"
      ],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_219fb0854b5c4dd195f3630fe7ce607a",
       "IPY_MODEL_47b6d4a4d95a4854887aa45e9446625c",
       "IPY_MODEL_9c952f336a3045da9ce304c8da4f81c5",
       "IPY_MODEL_ea6f75d7dae64f1f88f64ee7a2a75595",
       "IPY_MODEL_5f3865f5942e4f4596059a684566d280",
       "IPY_MODEL_881f6e8cf2cb4518a02e4fe3ae96f260"
      ],
      "layout": "IPY_MODEL_167e4351bff74792bea317b34d0156c1"
     }
    },
    "1251f7681af64d1695c4fff67f548863": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13ded88e4bce422b99a213975c22a679": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "167e4351bff74792bea317b34d0156c1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "219fb0854b5c4dd195f3630fe7ce607a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "FloatSliderView",
      "continuous_update": true,
      "description": "Knob",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_ddb813c3880c41ca8db7dc5acf60e696",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": ".2f",
      "step": 0.1,
      "style": "IPY_MODEL_237ea3c12e9f43489f6b3279f8e0b8a0",
      "value": 0
     }
    },
    "237ea3c12e9f43489f6b3279f8e0b8a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "",
      "handle_color": null
     }
    },
    "31e5dd27337740148248d2b82a49377a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "47b6d4a4d95a4854887aa45e9446625c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "TextModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "TextModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "TextView",
      "continuous_update": true,
      "description": "Prompt",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_1251f7681af64d1695c4fff67f548863",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_aba9e6e67ab5489abf867c1ebe855250",
      "value": "There exists"
     }
    },
    "5f3865f5942e4f4596059a684566d280": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Run Interact",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_13ded88e4bce422b99a213975c22a679",
      "style": "IPY_MODEL_869e4090b8eb4036a4dbb4acc8606ee4",
      "tooltip": ""
     }
    },
    "869e4090b8eb4036a4dbb4acc8606ee4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "881f6e8cf2cb4518a02e4fe3ae96f260": {
     "model_module": "@jupyter-widgets/output",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_db3b0ac319c940119df64fdeae233b5e",
      "msg_id": "",
      "outputs": []
     }
    },
    "9c952f336a3045da9ce304c8da4f81c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "legal",
       "military",
       "monsters",
       "politics",
       "positive_words",
       "religion",
       "science",
       "space",
       "technology"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Topic",
      "description_tooltip": null,
      "disabled": false,
      "index": 0,
      "layout": "IPY_MODEL_c350528a586b4720820c6b77aa250470",
      "style": "IPY_MODEL_eb5d3837f49045fa809bdfb7fe7236c1"
     }
    },
    "aba9e6e67ab5489abf867c1ebe855250": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c350528a586b4720820c6b77aa250470": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c7a1f3aeb10949c9899ff47b59cfd1b9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db3b0ac319c940119df64fdeae233b5e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ddb813c3880c41ca8db7dc5acf60e696": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea6f75d7dae64f1f88f64ee7a2a75595": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "fear",
       "joy",
       "anger",
       "sadness",
       "anticipation",
       "disgust",
       "surprise",
       "trust"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Affect",
      "description_tooltip": null,
      "disabled": false,
      "index": 0,
      "layout": "IPY_MODEL_c7a1f3aeb10949c9899ff47b59cfd1b9",
      "style": "IPY_MODEL_31e5dd27337740148248d2b82a49377a"
     }
    },
    "eb5d3837f49045fa809bdfb7fe7236c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
